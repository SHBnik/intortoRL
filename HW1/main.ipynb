{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RsVYVa1joR0D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "reward = np.array([[0, 0.2],\n",
        "                  [0, 0.2],\n",
        "                  [0, 0.2],\n",
        "                  [0, 0.2],\n",
        "                  [1, 0.2]])\n",
        "\n",
        "transition_prob = np.array([\n",
        "    [\n",
        "        [0, 0.8, 0.2, 0, 0],\n",
        "        [0, 0, 0.8, 0.2, 0],\n",
        "        [0, 0, 0.2, 0.8, 0],\n",
        "        [0, 0, 0, 0, 1],\n",
        "        [0, 0, 0, 0, 1]\n",
        "    ],\n",
        "    [\n",
        "        [0.9, 0.1, 0, 0, 0],\n",
        "        [0.9, 0.1, 0, 0, 0],\n",
        "        [0.9, 0, 0.1, 0, 0],\n",
        "        [0.9, 0, 0.1, 0, 0],\n",
        "        [0.9, 0, 0.1, 0, 0]\n",
        "    ]\n",
        "])\n",
        "\n",
        "initial_state = np.array([[1, 0, 0, 0, 0]])\n",
        "\n",
        "gamma = 0.95 #discount factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8JsG8xnoR0E"
      },
      "source": [
        "## DP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlICFrkHoR0G"
      },
      "source": [
        "To compute the optimal Q-values using Value Iteration, we have to do:\n",
        "\n",
        "1. Initialize the Q-values $Q(s, a)$ with zeros or some other initial values.\n",
        "2. Update the Q-values using the Bellman optimality equation for Q-values:\n",
        "\n",
        "$$\n",
        "Q^{*}(s, a) = \\sum_{s'} P(s'|s, a) \\left[ R(s, a) + \\gamma \\max_{a'} Q^{*}(s', a') \\right]\n",
        "$$\n",
        "\n",
        "3. Repeat step 2 until the Q-values converge (i.e., the maximum change in Q-values between consecutive iterations is smaller than a predefined threshold).\n",
        "\n",
        "Once the process converges, the optimal Q-values $Q^*(s, a)$ are obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcoeX9bNoR0H",
        "outputId": "3b084379-94c5-47d1-99f6-dd35ad3097e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q table DP: \n",
            "[[16.42770906 15.87575943]\n",
            " [17.15862264 15.87575943]\n",
            " [17.82714116 15.93926869]\n",
            " [18.99998066 15.93926869]\n",
            " [19.99998066 15.93926869]]\n"
          ]
        }
      ],
      "source": [
        "def Q_value_DP(reward, transition_prob, gamma):\n",
        "    q_table = np.zeros((5,2))\n",
        "    while True:\n",
        "        q_table_new = np.copy(q_table)#np.zeros((5,2))\n",
        "        for s in range(5):\n",
        "            for a in range(2):\n",
        "                q_table_new[s ,a] = np.sum(transition_prob[a, s, :] * (reward[s, a] + gamma * np.max(q_table, axis=1)))\n",
        "\n",
        "        if np.max(np.abs(q_table - q_table_new)) < 1e-6:\n",
        "            break\n",
        "        q_table = q_table_new\n",
        "    \n",
        "    return q_table\n",
        "\n",
        "q_value_dp = Q_value_DP(reward, transition_prob, gamma)\n",
        "print(\"Q table DP: \")\n",
        "print(q_value_dp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHOMQZzfoR0J"
      },
      "source": [
        "## Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH-b1OmKoR0K"
      },
      "source": [
        "\n",
        "The Q-learning update rule is given as:\n",
        "\n",
        "$$\n",
        "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
        "$$\n",
        "\n",
        "For the Q-learning algorithm we are following the following steps:\n",
        "\n",
        "1. Initialize the Q-values $Q(s, a)$ with zeros.\n",
        "2. For each episode, start from the initial state ($s_0$) and follow these steps:\n",
        "   a. Choose an action $a$ from the current state $s$ using epsilon-greedy.\n",
        "   b. Take the action $a$, observe the reward $r$ and the next state $s'$.\n",
        "   c. Update the Q-value using the Q-learning update rule.\n",
        "   d. Set the current state $s$ to the next state $s'$.\n",
        "   e. Repeat steps a-d until the max_episode times (because we dont have terminal state).\n",
        "\n",
        "3. Continue running episodes until reach maximum number of episodes.\n",
        "\n",
        "After the Q-learning algorithm converges, the optimal Q-values $Q^*(s, a)$ are obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV5Bfn7FoR0K",
        "outputId": "bbdd1bb5-f640-4833-e41e-7b1326967f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q table Q-Learning: \n",
            "[[16.45047984 15.73941514]\n",
            " [17.25684472 15.78435154]\n",
            " [17.67300918 15.8645954 ]\n",
            " [19.         15.85974815]\n",
            " [20.         15.93704007]]\n"
          ]
        }
      ],
      "source": [
        "def epsilon_greedy(q_values, state, epsilon):\n",
        "    # print(len(q_values[state]))\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(len(q_values[state]))\n",
        "    else:\n",
        "        return np.argmax(q_values[state])\n",
        "\n",
        "def Q_learning(reward, gamma, alpha, initial_state = 0, max_episode = 1000, max_step = 1000):\n",
        "    q_table = np.zeros((5,2))\n",
        "    for episod in range(max_episode):\n",
        "        state = initial_state\n",
        "        for step in range(max_step):\n",
        "            action = epsilon_greedy(q_table, state, 0.1)\n",
        "            state_new = np.random.choice(5, p=transition_prob[action, state])                    \n",
        "            q_table[state, action] = q_table[state, action] + alpha *(reward[state, action] + gamma * np.max(q_table[state_new]) - q_table[state, action]) \n",
        "            state = state_new\n",
        "\n",
        "    return q_table\n",
        "\n",
        "q_learning = Q_learning(reward, gamma, 0.1)\n",
        "print(\"Q table Q-Learning: \")\n",
        "print(q_learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t3guRVHoR0L"
      },
      "source": [
        "## Deference between DP and Q-Learning resluts\n",
        "The difference between DP and Q-Learning results is quite small, as we can see. The reason for this difference is that for Q-Learning, we ran the algorithm for only 1000 episodes with 1000 steps each. If we were to increase the number of episodes and steps, the difference between the two algorithms should become smaller and smaller."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}